{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "agent_motion_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivi9DpEbgJJe"
      },
      "source": [
        "!pip install l5kit\n",
        "!pip install -U PyYAML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRQOrAdwJPPm"
      },
      "source": [
        "dataPath = \"/content/gdrive/My Drive/Assignments/18797/lyftFormatColab\"\n",
        "#This folder should contain sample.tar and semantic_map.tar\n",
        "#Upload agent_motion_config.yaml to runtime in /content/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8O4IjGBv_4F"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50\n",
        "from tqdm import tqdm\n",
        "\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8O6bbrDwxeM"
      },
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "shutil.unpack_archive(os.path.join(dataPath, \"sample.tar\"), \"sample\")\n",
        "shutil.unpack_archive(os.path.join(dataPath, \"semantic_map.tar\"), \"sample/semantic_map\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nqGynmg1cjV"
      },
      "source": [
        "# set env variable for data\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = \"sample\"\n",
        "dm = LocalDataManager(None)\n",
        "# get config\n",
        "cfg = load_config_data(\"agent_motion_config.yaml\")\n",
        "print(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4M1D9yzB-NB"
      },
      "source": [
        "def build_model(cfg: Dict) -> torch.nn.Module:\n",
        "    # load pre-trained Conv2D model\n",
        "    model = resnet50(pretrained=True)\n",
        "\n",
        "    # change input channels number to match the rasterizer's output\n",
        "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "    num_in_channels = 3 + num_history_channels\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        num_in_channels,\n",
        "        model.conv1.out_channels,\n",
        "        kernel_size=model.conv1.kernel_size,\n",
        "        stride=model.conv1.stride,\n",
        "        padding=model.conv1.padding,\n",
        "        bias=False,\n",
        "    )\n",
        "    # change output size to (X, Y) * number of future states\n",
        "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
        "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRehNLTBB_eC"
      },
      "source": [
        "def forward(data, model, device, criterion):\n",
        "    inputs = data[\"image\"].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    targets = data[\"target_positions\"].to(device)\n",
        "    # Forward pass\n",
        "    outputs = model(inputs).reshape(targets.shape)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
        "    loss = loss * target_availabilities\n",
        "    loss = loss.mean()\n",
        "    return loss, outputs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ZdIoqwCDNM"
      },
      "source": [
        "train_cfg = cfg[\"train_data_loader\"]\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "train_cfg[\"key\"] = \"/content/sample/sample.zarr\"\n",
        "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
        "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
        "                             num_workers=train_cfg[\"num_workers\"])\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrJmB61-KSgw"
      },
      "source": [
        "# ==== INIT MODEL\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = build_model(cfg).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss(reduction=\"none\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35awcjOUKYN5",
        "outputId": "1e500064-b187-4c2a-ba73-c8d124daccf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "for _ in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    loss, _ = forward(data, model, device, criterion)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses_train.append(loss.item())\n",
        "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 168.7838592529297 loss(avg): 81.97041282653808: 100%|██████████| 5/5 [00:59<00:00, 11.96s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}